# BigDataProject1

** Step 1: Create two Datasets! Write a java program (that creates two datasets (each is stored in an independent file!); Buyers and Purchases. Each line in Buyers file represents a single Buyer, where each line in Purchases file represents a single Purchase. The attributed values within each line are comma separated. The Buyers dataset should have the following attributes for each Buyer : BuyerID: unique sequential number from 1 to 10,000 (meaning, the file has 10,000 buyers). BuyerName: random sequence of characters of length between 10 and 15 (make sure that you exclude comma from the possible generated characters!) BuyerAge: random integer number between 12 to 75 BuyerGender: randomly generated string that is either “male” or “female” BuyerSalary: random float number between 3500 and 11000 The Purchases dataset should have the following attributes for each Purchase : purchID: unique sequential integer number from 1 to 1,000,000 (the file has 1M purchases). BuyerID: References one of the buyers IDs, i.e., from 1 to 10,000 (on Avg. a buyer has 100 purchases.) purchPrice: random float number between 10 and 100 purchNumItems: random integer number between 1 and 10 Note: This task is writing a regular java program, not a mapReduce Job! 

Step 2: Upload the created dataset into HDFS! use one of the Hadoop commands (either put or copyFromLocal) to upload the datasets into HDFS.. Then open Hadoop’s Web UI (inside the docker container) and make sure that it is successfully uploaded!

Step 3: Writing mapReduce Jobs! First Job,Write a job that reports the buyers whose Age between 20 and 50. Write a job that reports for every buyer, the number of purchases that he made (i.e., count his purchases) and the sum of the prices of these purchases. The output should have one line for each buyer containing: BuyerID, numPurchases, sumPrices  You are required to use a Combiner in this job.
